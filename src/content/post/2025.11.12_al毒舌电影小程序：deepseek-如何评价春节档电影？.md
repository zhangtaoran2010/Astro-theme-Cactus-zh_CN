---
title: Al毒舌电影小程序：DeepSeek 如何评价春节档电影？
description: 这是一篇有意思的文章
publishDate: 2025-11-12
ogImage: /social-card.avif
---
**这个教程为第一个实战教程，目前暂无手把手的视频教程。之后会补上！**

**一.作品展示**

本期视频我们将会构建一个AI电影小程序！通过DeepSeek + Coze作为后端，来评价今年春节档的电影。

小程序首页会展示春节档的电影评分和票房，电影详情页面会请求Coze的后端 API，实现Al毒舌评论春节档的效果。并且你还可以通过聊天对话框和AI对话，了解这一部电影的更多细节。

**• 小程序首页：展现春节档电影**

![](/assets/images/image511.png)

**• 电影详情页：AI智能评价春节档电影＋和AI对话**

![](/assets/images/image512.png)

**二. 项目流程**

![](/assets/images/截屏2025-11-12-16.22.20.png)

**三. 有哪些技术栈？**

![](/assets/images/截屏2025-11-12-16.22.40.png)

**四. 实战环节**

**提示：AI生成的prompt效果不一定100%稳定。如果通过我的prompt在Cursor中无法100%复现效果，你可以：**

**1. 自己主动的追问，来达到效果**

**2. 或者直接使用我给出代码**

**建议使用第一种方式！AI时代得学会主动和AI沟通，来达成自己想要的效果。**

**1. 爬虫豆瓣电影评论**

**1.1 使用Cursor编写爬虫代码**

首先我们需要通过[豆瓣电影](https://movie.douban.com/)获取到春节档电影的热门评论。这一步我们将使用Curosr来实现！

![](/assets/images/image513.png)

以哪吒为例，我们主要是获取的这个页面

(https://movie.douban.com/subject/34780991/comments) 的评论数据：

![](/assets/images/image514.png)

首先我们打开Curosr， 然后在COMPOSER中输入以下提示词：

```
完成一个获取豆瓣电影热门短评的python爬虫代码，命名为hot_comments.py 
- 在项目根目录直接创建文件并给出详细的代码
- headers 给出 cookie 变量，我需要手动填写
- 获取所有热门短评，max_page默认19
```

你会得到下面所示的Python代码：

hot_comments.py:

```
      
import requests
from bs4 import BeautifulSoup
import time
import random
import json
import os
from typing import List, Dict

class DoubanHotCommentsCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Cookie': '', # 需要手动填入你的豆瓣Cookie
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Host': 'movie.douban.com',
            'Referer': 'https://movie.douban.com'
        }
        self.base_url = "https://movie.douban.com/subject/{}/comments"
        
    def get_hot_comments(self, movie_id: str, max_pages: int = 19) -> List[Dict]:
        """获取指定电影的热门短评"""
        comments = []
        for page in range(max_pages):
            url = f"{self.base_url.format(movie_id)}?start={page * 20}&limit=20&status=P&sort=new_score"
            try:
                response = requests.get(url, headers=self.headers)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    comment_items = soup.find_all('div', class_='comment-item')
                    
                    if not comment_items:
                        print(f"第 {page + 1} 页没有更多评论")
                        break
                        
                    for item in comment_items:
                        comment_data = self._parse_comment(item)
                        if comment_data:
                            comments.append(comment_data)
                            
                    print(f"成功爬取第 {page + 1} 页，当前共 {len(comments)} 条评论")
                            
                # 添加随机延迟，避免被封
                time.sleep(random.uniform(3, 10))
            except Exception as e:
                print(f"爬取第 {page + 1} 页时出错: {str(e)}")
                
        return comments
    
    def _parse_comment(self, comment_item) -> Dict:
        """解析单条短评数据"""
        try:
            # 获取评论者信息
            author = comment_item.find('span', class_='comment-info').find('a').get_text(strip=True)
            
            # 获取评分
            rating_span = comment_item.find('span', class_=['allstar50', 'allstar40', 'allstar30', 'allstar20', 'allstar10'])
            rating = rating_span['title'] if rating_span else "未评分"
            
            # 获取评论内容
            content = comment_item.find('span', class_='short').get_text(strip=True)
            
            # 获取评论时间
            time_str = comment_item.find('span', class_='comment-time').get_text(strip=True)
            
            # 获取点赞数
            votes = comment_item.find('span', class_='votes').get_text(strip=True)
            
            # 获取是否购票
            ticket_span = comment_item.find('span', class_='comment-info').find('span', class_='user-ticket')
            ticket = ticket_span.get_text(strip=True) if ticket_span else "未标注"
            
            return {
                'author': author,
                'rating': rating,
                'content': content,
                'time': time_str,
                'votes': votes,
                'ticket': ticket
            }
        except Exception as e:
            print(f"解析评论时出错: {str(e)}")
            return None
    
    def save_comments(self, comments: List[Dict], movie_id: str):
        """保存评论到JSON文件"""
        if not os.path.exists('hot_comments'):
            os.makedirs('hot_comments')
            
        filename = f'hot_comments/movie_{movie_id}_hot_comments.json'
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(comments, f, ensure_ascii=False, indent=2)
        print(f"评论已保存到 {filename}")

def main():
    crawler = DoubanHotCommentsCrawler()
    movie_id = ""  
    comments = crawler.get_hot_comments(movie_id)
    crawler.save_comments(comments, movie_id)

if __name__ == "__main__":
    main() 

 
```

这段代码中有2个需要你主动去修改的地方，这里有细节需要注意：

**1. 增加cookie**

首先我们需要在这里手动填入你的cookie，cookie是你用户身份的标识，填入了才能避免你的爬虫请求不会被豆瓣列为恶意请求给禁用掉。

![](/assets/images/image515.png)

从浏览器中获取cookie的步骤可以参考下图

 • 鼠标右键，打开「检查」

• 点击网络，然后点击右侧的任意请求

• 找到你的Cookie，复制粘贴到刚才代码中的地方

![](/assets/images/image516.png)

**2. 增加 movie_id**

第二步是需要我们获取电影的id，填入到下图所在的地方

![](/assets/images/image517.png)

这一步很简单，因为豆瓣的电影id都是在链接上面公开可见的，以哪吒为例，这个 **34780991** 就是哪吒的电影id。复制并粘贴到刚才的代码中

![](/assets/images/image518.png)

到这一步我们就获取到了可以爬取豆瓣电影的Python代码！

**1.2 运行Python代码，爬虫**

**1.2.1 安装依赖**

接下来在Cursor中打开命令行，首先我们需要安装对应的Python依赖，因为我们的项目文件开头依赖了一些库。

![](/assets/images/image519.png)

但是，我相信你不知道哪些库是需要安装的，因为有一些可能是Python标准库无需安装？那怎么知道我需要安装哪些库呢？很简单，在Cursor中问就好。你打开Cursor，然后提问：请你告诉我需要安装哪一些依赖？

![](/assets/images/image520.png)

然后直接点击 Run 安装就好

**1.2.2 运行爬虫代码**

安装好依赖，我们就可以运行Python的代码，执行爬虫啦！

在Cursor中打开命令行，输入以下的命令执行爬虫代码

执行python命令:

```
python hot_comments.py
```

如果顺利的话，你应该就能看到执行成功，并且生成的json文件也在目录下面了。

![](/assets/images/image521.png)

我们可以打开目录看一下，应该获取到了前20页的评论数据。如果你想要获取更多，可以修改爬虫代码里面的页码数。

![](/assets/images/image522.png)

**1.3 重复执行不同电影的爬虫**

接下来我们可以重复执行不同电影的爬虫代码，只需要修改对应的电影id，然后将获取到的所有JSON文件，重新命名成为我们熟悉的中文电影名称。如图所示：

![](/assets/images/image523.png)

到这一步我们就基本完成了春节档豆瓣电影的爬虫了人*！

使用Cursor进行爬虫的开发是不是很简单呢？我获取到的所有的阶层数据可以在下方获取哦：1.春节档电影评论的爬虫数据！



**2. 将评论数据给DeepSeek**
